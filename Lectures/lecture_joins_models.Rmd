---
title: "Joins, Split-Apply-Combine & MCPs"
author: "Abhijit Dasgupta"
date: "October 24, 2018"
output:
  revealjs::revealjs_presentation:
    theme: sky
    highlight: zenburn
    self_contained: false
    center: false
    reveal_plugins: ["notes","search", "chalkboard"]
    reveal_options:
      chalkboard:
        theme: whiteboard
---

```{r setup, include=FALSE, message = F, warning = F}
knitr::opts_chunk$set(echo = FALSE, message = F, warning = F)
library(tidyverse)
```

# Goals today

## Goals today 

+ Learn how to join data sets (merging)
+ Split-apply-combine
    + Split a dataset into a list of several datasets
    + Do something to each dataset
    + Put the results back together
+ Use it for 
    + Running tests for many variables
    + Visualizing data with p-value annotation
+ Understand why we need multiple comparison procedures (MCP)
    + Things to think about

<aside>
We need to 

+ put datasets capturing different attributes together to find a complete picture
+ evaluate different attributes to see if they contribute to our understanding
+ hedge our bets to ensure we find 

</aside>
    
# Data


    
# Joins

## Putting data sets together

+ Quite often, data on individuals lie in different tables
    - Clinical, demographic and bioinformatic data
    - Drug, procedure, and payment data (think Mediare)
    - Personal health data across different healthcare entities

## Joining data sets

We'll assume that

+ both datasets are rectangular
+ there is at least one common variable between the two datasets

    - ID number
    - Social Security Number
    - Identifiable information
    
# Multiple comparison procedures (MCP)

## Why do we need it?

+ Recall, in hypothesis tests, the Type I error (or false positive rate) is 
$$\Pr(Reject\ H_0 | H_0\ is\ true)$$
This is typically limited by the testing procedure to 5%.  
    - For the more technically interested, this is from the _Neyman-Pearson lemma_
+ There is always a chance we are wrong!!

## Why do we need it?

Imagine your test is like a biased coin, with heads being "Reject H~0~" and tails being "Do not reject H~0~"

Now assume H~0~ is true, and you're doing multiple tests using the same data

```{r}
tbl <- tribble(~`Number of tests`, ~`Coin tosses` , ~`Pr(at least one head)`,
               format(1, scientific = F), '1 toss', round(1-(0.95)^1,2),
               format(2, scientific = F), '2 tosses', round(1-(0.95)^2, 2),
               format(5, scientific=F), '5 tosses', round(1-(0.95)^5,2),
               format(10, scientific = F), '10 tosses', round(1-(0.95)^10,2),
               format(100, scientific = F), '100 tosses', round(1-(0.95)^100, 2),
               format(1000000, big.mark=",", scientific = F), '1 million tosses', round(1-(0.95)^100000, 2))
knitr::kable(tbl)
```

## Bonferroni correction

If you have _n_ tests using the same data, then make sure that the Type I error is $0.05/n$


