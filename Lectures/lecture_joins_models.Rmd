---
title: "Tests, Joins and Models"
author: "Abhijit Dasgupta"
date: "October 24, 2018"
output:
  revealjs::revealjs_presentation:
    theme: sky
    highlight: zenburn
    self_contained: false
    center: false
    reveal_plugins: ["notes","search", "chalkboard"]
    reveal_options:
      chalkboard:
        theme: whiteboard
---

```{r setup, include=FALSE, message = F, warning = F}
knitr::opts_chunk$set(echo = FALSE, message = F, warning = F)
library(tidyverse)
```

# Multiple comparison procedures (MCP)

## Why do we need it?

+ Recall, in hypothesis tests, the Type I error (or false positive rate) is 
$$\Pr(Reject\ H_0 | H_0\ is\ true)$$
This is typically limited by the testing procedure to 5%.  
    - For the more technically interested, this is from the _Neyman-Pearson lemma_
+ There is always a chance we are wrong!!

## Why do we need it?

Imagine your test is like a biased coin, with heads being "Reject H~0~" and tails being "Do not reject H~0~"

Now assume H~0~ is true, and you're doing multiple tests using the same data

```{r}
tbl <- tribble(~`Number of tests`, ~`Coin tosses` , ~`Pr(at least one head)`,
               format(1, scientific = F), '1 toss', round(1-(0.95)^1,2),
               format(2, scientific = F), '2 tosses', round(1-(0.95)^2, 2),
               format(5, scientific=F), '5 tosses', round(1-(0.95)^5,2),
               format(10, scientific = F), '10 tosses', round(1-(0.95)^10,2),
               format(100, scientific = F), '100 tosses', round(1-(0.95)^100, 2),
               format(1000000, big.mark=",", scientific = F), '1 million tosses', round(1-(0.95)^100000, 2))
knitr::kable(tbl)
```

## Bonferroni correction

If you have _n_ tests using the same data, then make sure that the Type I error is $0.05/n$


