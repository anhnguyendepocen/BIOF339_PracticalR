---
title: "Practicum 5 Solution"
author: "Eugen Buehler"
date: "October 10, 2017"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


1. Create a linear model from the "swiss" data set (in the datasets package), predicting Fertility based on Agriculture.

```{r}
library(datasets)
aggModel <- lm(Fertility ~ Agriculture, data=swiss)
summary(aggModel)
```

Now build a model predicting Fertility based on Education

```{r}
educationModel <- lm(Fertility ~ Education, data=swiss)
summary(educationModel)
```

and finally a model using both variables.

```{r}
jointModel <- lm(Fertility ~ Education + Agriculture, data=swiss)
summary(jointModel)
```

Use anova to ask if the combination of Agriculture and Education is statistically significantly better than Education alone.

```{r}
anova(educationModel, jointModel, test="Chisq")
```

Since the p-value is not very small, we can't conclude that adding the Agriculture variable significantly improves the model.

Finally, use the step function to quickly find an optimal model for predicting Fertility, selecting features from all the predictive variables.  The easiest way to do this will be to set "object" equal to a model containing all the other variables (lm(Fertility ~ .,data=swiss)) with no other function options specified.

```{r}
stepModel <- step(lm(Fertility ~ .,data=swiss))
summary(stepModel)
```

Note that only one variable gets dropped from the model.

2. Using the "read.csv" function, read in a data frame of data about graduate students acceptance (https://stats.idre.ucla.edu/stat/data/binary.csv).  Build a logistic regression model that predicts acceptance based on the predictive variable GRE and GPA.  Do these two predictive variable seem to have equal importance in the prediction?  How do you know?  (Adapted from the web tutorial at https://stats.idre.ucla.edu/r/dae/logit-regression/)

```{r}
gradData <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
summary(glm(formula = admit ~ gre + gpa, family = "binomial", data = gradData))
```
Although the size of the parameter estimates for these two variables are very different, recall that they have very different scales.  To compare, we would need to normalize (subtract the mean and divide by the standard deviation).

```{r}
gradData$greZ <- (gradData$gre - mean(gradData$gre)) / sd(gradData$gre)
gradData$gpaZ <- (gradData$gpa - mean(gradData$gpa)) / sd(gradData$gpa)
summary(glm(formula = admit ~ greZ + gpaZ, family = "binomial", data = gradData))
```

After normalization, the parameter estimates for the two predictors are very close (overlaping when considering the standard error of the parameters) , and so we can conclude that they have roughly equal weight.

