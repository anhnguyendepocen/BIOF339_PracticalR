---
title: "Practicum 5 Solution"
author: "Eugen Buehler"
date: "October 10, 2017"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1. Install the plsgenomics package.  We will use a set of gene expression data from normal and tumor tissue in this package to practice using multiple comparison procedures.  To load the data and perform a wilcox test on each gene (is the median tumor gene expression significantly different than the median normal gene expresison?), execute the following commands:

```{r warning=FALSE}
library(plsgenomics)
data(Colon)
colon.pvals <- sapply(1:length(Colon$X[1,]), 
                      function(x) wilcox.test(Colon$X[,x] ~ Colon$Y)$p.value)
```

Don't worry if the use of "sapply" seems complicated.  We will cover this powerful command in another lecture.  Now we have 2000 p-values from 2000 wilcox tests on 2000 different genes.  How many appear signficant at an alpha of 0.01?

```{r}
sum(colon.pvals < 0.01)
```

After applying a bonferonni correction, how many are still significant at the same alpha?

```{r}
sum(p.adjust(colon.pvals, method="bonferroni") < 0.01)
```

If we wish instead to use a false discovery rate of 1 in 100 (0.01), how many genes would pass this test?

```{r}
sum(p.adjust(colon.pvals, method="fdr") < 0.01)
```

2. Create a linear model from the "swiss" data set (in the datasets package), predicting Fertility based on Agriculture.

```{r}
library(datasets)
aggModel <- lm(Fertility ~ Agriculture, data=swiss)
summary(aggModel)
```

Now build a model predicting Fertility based on Education

```{r}
educationModel <- lm(Fertility ~ Education, data=swiss)
summary(educationModel)
```

and finally a model using both variables.

```{r}
jointModel <- lm(Fertility ~ Education + Agriculture, data=swiss)
summary(jointModel)
```

Use anova to ask if the combination of Agriculture and Education is statistically significantly better than Education alone.

```{r}
anova(educationModel, jointModel, test="Chisq")
```

Since the p-value is not very small, we can't conclude that adding the Agriculture variable significantly improves the model.

Finally, use the step function to quickly find an optimal model for predicting Fertility, selecting features from all the predictive variables.  The easiest way to do this will be to set "object" equal to a model containing all the other variables (lm(Fertility ~ .,data=swiss)) with no other function options specified.

```{r}
stepModel <- step(lm(Fertility ~ .,data=swiss))
summary(stepModel)
```

Note that only one variable gets dropped from the model.

3. Using the "read.csv" function, read in a data frame of data about graduate students acceptance (https://stats.idre.ucla.edu/stat/data/binary.csv).  Build a logistic regression model that predicts acceptance based on the predictive variable GRE and GPA.  Do these two predictive variable seem to have equal importance in the prediction?  How do you know?  (Adapted from the web tutorial at https://stats.idre.ucla.edu/r/dae/logit-regression/)

```{r}
gradData <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
summary(glm(formula = admit ~ gre + gpa, family = "binomial", data = gradData))
```
Although the size of the parameter estimates for these two variables are very different, recall that they have very different scales.  To compare, we would need to normalize (subtract the mean and divide by the standard deviation).

```{r}
gradData$greZ <- (gradData$gre - mean(gradData$gre)) / sd(gradData$gre)
gradData$gpaZ <- (gradData$gpa - mean(gradData$gpa)) / sd(gradData$gpa)
summary(glm(formula = admit ~ greZ + gpaZ, family = "binomial", data = gradData))
```

After normalization, the parameter estimates for the two predictors are very close (overlaping when considering the standard error of the parameters) , and so we can conclude that they have roughly equal weight.

